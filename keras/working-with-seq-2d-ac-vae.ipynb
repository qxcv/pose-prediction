{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval code for action-based VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# enable access to action-from-pose model\n",
    "si_path = '/home/sam/repos/structuredinference/'\n",
    "if si_path not in sys.path:\n",
    "    sys.path.append(si_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import plot_confusion_matrix\n",
    "from generate_seq_2d_ac_vae import load_data, parser\n",
    "from common_pp.act_class_model import make_model\n",
    "from common_pp.act_pre_common import one_hot_cat, classifier_transform, balance_aclass_ds, merge_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('seq-2d-ac-vae/config.json', 'r') as fp:\n",
    "    config = json.load(fp)\n",
    "    # might actually be easier to just parse the arguments if I need anything else\n",
    "    data_file = None\n",
    "    args = parser.parse_args(config['args'])\n",
    "    data_file = args.data_file\n",
    "    seq_length = args.seq_length\n",
    "    seq_skip = args.seq_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_load(path):\n",
    "    print('Loading', path)\n",
    "    return load_model(path)\n",
    "act_encoder = print_load(config['act_encoder_path'])\n",
    "pose_encoder = print_load(config['pose_encoder_path'])\n",
    "decoder = print_load(config['decoder_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _, db = load_data(data_file, seq_length, seq_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expt 1: Classifying generated poses\n",
    "\n",
    "See whether correct actions can be recovered from samples taken from the model (by training separate predictor to look at action-conditioned poses). This is a \"minimum standard\" for success (and one which the DMM does not meet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_aclass_ds = db.get_aclass_ds(train=True)\n",
    "val_aclass_ds = db.get_aclass_ds(train=False)\n",
    "merge_map = {\n",
    "    'attach leg 1': '*tach',\n",
    "    'attach leg 2': '*tach',\n",
    "    'attach leg 3': '*tach',\n",
    "    'attach leg 4': '*tach',\n",
    "    'detach leg 1': '*tach',\n",
    "    'detach leg 2': '*tach',\n",
    "    'detach leg 3': '*tach',\n",
    "    'detach leg 4': '*tach',\n",
    "    'spin in': 'spin',\n",
    "    'spin out': 'spin',\n",
    "    'n/a': None\n",
    "}\n",
    "# note that we're using unbalanced datasets\n",
    "_, train_aclass_ds \\\n",
    "    = merge_actions(train_aclass_ds, merge_map, db.action_names)\n",
    "aclass_target_names, val_aclass_ds \\\n",
    "    = merge_actions(val_aclass_ds, merge_map, db.action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_act_inflow(merged_idx):\n",
    "    \"\"\"How many original actions map to the given merged\n",
    "    action?\"\"\"\n",
    "    merged_name = aclass_target_names[merged_idx]\n",
    "    return len({\n",
    "            k for k, v in merge_map.items() if v == merged_name\n",
    "        })\n",
    "\n",
    "def to_merged_idx(orig_idx):\n",
    "    \"\"\"What is the index of the merged action which the given\n",
    "    original action points to.\"\"\"\n",
    "    orig_name = db.action_names[orig_idx]\n",
    "    new_name = merge_map.get(orig_name, orig_name)\n",
    "    if new_name is None:\n",
    "        return None\n",
    "    return aclass_target_names.index(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_samples(samples_per_act):\n",
    "    gen_Z = []\n",
    "    gen_Y = []\n",
    "    noise_dim = decoder.input_shape[-1]\n",
    "    for act_idx in range(db.num_actions):\n",
    "        merged_idx = to_merged_idx(act_idx)\n",
    "        if merged_idx is None:\n",
    "            # skip this action because it was removed\n",
    "            continue\n",
    "        inflow = merge_act_inflow(merged_idx)\n",
    "        to_generate = int(np.ceil(samples_per_act / float(inflow)))\n",
    "        mu, cov = act_encoder.predict(np.asarray([[act_idx] * seq_length]))\n",
    "        nzs = np.random.randn(to_generate, noise_dim)\n",
    "        zs = nzs * cov + mu\n",
    "        gen_Z.extend(zs.tolist())\n",
    "        gen_Y.extend([merged_idx] * to_generate)\n",
    "\n",
    "    gen_Z = np.asarray(gen_Z)\n",
    "    gen_Y = one_hot_cat(np.asarray(gen_Y), len(aclass_target_names))\n",
    "    assert np.all(np.isfinite(gen_Z))\n",
    "    assert np.all(np.isfinite(gen_Y))\n",
    "\n",
    "    gen_X = classifier_transform(decoder.predict(gen_Z, batch_size=1024))\n",
    "    assert np.all(np.isfinite(gen_X))\n",
    "\n",
    "    return gen_X, gen_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_X, gen_Y = make_samples(8192)\n",
    "val_X, val_Y = make_samples(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('gen_X:', gen_X.shape, 'gen_Y:', gen_Y.shape,\n",
    "      'val_X:',  val_X.shape, 'val_Y:', val_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now try to fit a model to the recovered poses\n",
    "model = make_model(gen_X.shape[1], gen_X.shape[2], len(aclass_target_names))\n",
    "model.fit(gen_X,\n",
    "          gen_Y,\n",
    "          batch_size=64,\n",
    "          nb_epoch=2,\n",
    "          validation_data=(val_X, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(val_X, batch_size=10000).argmax(axis=-1)    \n",
    "print('Results for sampled, action-conditioned poses:')\n",
    "print(classification_report(val_Y.argmax(axis=-1), Y_pred, target_names=aclass_target_names))\n",
    "cm = confusion_matrix(val_Y.argmax(axis=-1), Y_pred)\n",
    "plot_confusion_matrix(cm, aclass_target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing with the \"real\" action classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_action_model_path = path.join(si_path, 'expt-ikeadb/chkpt-aclass/action-classifier-50-0.75.hdf5')\n",
    "old_action_model = load_model(old_action_model_path)\n",
    "sample_class_actions = old_action_model.output_shape[1]\n",
    "sample_class_indim = old_action_model.input_shape[-1]\n",
    "action_model = make_model(db.seq_length - 1,\n",
    "                          sample_class_indim,\n",
    "                          sample_class_actions)\n",
    "action_model.set_weights(old_action_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = action_model.predict(val_X, batch_size=10000).argmax(axis=-1)    \n",
    "print('Results for sampled, action-conditioned poses:')\n",
    "print(classification_report(val_Y.argmax(axis=-1), Y_pred, target_names=aclass_target_names))\n",
    "cm = confusion_matrix(val_Y.argmax(axis=-1), Y_pred)\n",
    "plot_confusion_matrix(cm, aclass_target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expt 2: realism on action classifier dataset\n",
    "\n",
    "Check whether pose sequences passed through the encoder are as realistic (in terms of recovered actions) as the corresponding originals. Will have to use action classification dataset for this.\n",
    "\n",
    "Note that the input here is poses K, K-1 through 1, and the output is poses 1 through K. No actions are used at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expt 3: overlaying poses\n",
    "\n",
    "Try to do some completions. Layer them on top of the original videos so that I can see whether my model is actually working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pose-prediction]",
   "language": "python",
   "name": "conda-env-pose-prediction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
